PROJECT STRUCTURE + AUDIO MODULE ONBOARDING

------------------------------------------------------------
PROJECT ROOT (actual repo layout)
------------------------------------------------------------
Back-Hackathon2025/
│
├── main.py                          (desktop bootstrap, launches Tauri)
├── api/                             (FastAPI backend)
│   └── server.py
├── command_controller/              (command parsing + execution)
│   ├── controller.py
│   ├── engine.py
│   ├── llm.py
│   └── executor.py
├── gesture_module/                  (runtime gesture workflow)
│   ├── workflow.py
│   └── gesture_recognizer.py
├── video_module/                    (gesture dataset + TFLite pipeline)
│   ├── gesture_ml.py
│   ├── tflite_pipeline.py
│   ├── tflite_classifiers.py
│   └── video_stream.py
├── voice_module/                    (voice capture + STT)
│   ├── voice_listener.py
│   ├── stt_engine.py
│   ├── stt_whisper_local.py
│   └── voice_utils.py
├── webui/                           (React + Vite UI in Tauri)
│   ├── src/
│   └── src-tauri/
├── config/                          (JSON settings)
│   ├── app_settings.json
│   ├── gesture_config.json
│   ├── command_settings.json
│   └── command_map.json
├── data/                            (preset datasets/models)
├── user_data/                       (per-user datasets + models)
├── utils/                           (shared helpers)
├── scripts/                         (build + tooling)
└── hand-gesture-recognition-mediapipe/ (reference repo)

------------------------------------------------------------
AUDIO MODULE TEAM ONBOARDING
------------------------------------------------------------

Goal:
Implement or improve Speech-to-Text (STT) providers and make sure transcripts
are forwarded into the command pipeline.

Key integration points:
1) VoiceListener -> SpeechToTextEngine -> CommandController
2) Optional API endpoints if you want UI-controlled start/stop

------------------------------------------------------------
CURRENT VOICE FLOW (code path)
------------------------------------------------------------

voice_module/voice_listener.py
- Owns microphone capture + streaming in a background thread.
- Uses SpeechToTextEngine to convert audio into a transcript.
- Forwards transcript to CommandController:
  controller.handle_event(source="voice", action=transcription)

voice_module/stt_engine.py
- Provider switch based on STT_PROVIDER env:
  - "whisper-local"
- Expected public method:
  async transcribe_stream(audio_stream) -> str transcript

command_controller/controller.py
- Receives the transcript as "action" for source="voice".
- Sends it through the LLM intent parser + executor.

------------------------------------------------------------
WHERE TO SEND STT OUTPUT (MOST IMPORTANT)
------------------------------------------------------------

Your STT engine must ultimately call this:

CommandController.handle_event(
    source="voice",
    action=transcript_text,
    payload=None
)

This is the only required integration to get voice commands executed.

------------------------------------------------------------
OPTIONAL: API ENDPOINTS FOR VOICE CONTROL
------------------------------------------------------------

There is currently no FastAPI endpoint for voice start/stop. If the UI team
wants voice control, add endpoints in api/server.py:

Suggested endpoints:
- POST /voice/start  -> starts VoiceListener in background
- POST /voice/stop   -> stops VoiceListener
- GET  /voice/status -> returns running state + provider

Where to wire:
- api/server.py should own a module-level VoiceListener instance.
- Use settings from config/app_settings.json for device selection.

------------------------------------------------------------
SETTINGS + ENVIRONMENT (voice-related)
------------------------------------------------------------

Environment variables (STT providers):
- STT_PROVIDER = whisper-local
- LOCAL_WHISPER_MODEL_PATH, LOCAL_WHISPER_DEVICE, LOCAL_WHISPER_COMPUTE_TYPE, LOCAL_WHISPER_LANGUAGE
- LOCAL_WHISPER_MODEL_PATH, LOCAL_WHISPER_DEVICE, LOCAL_WHISPER_COMPUTE_TYPE

Runtime settings:
- config/app_settings.json
  - microphone_device_index
  - speaker_device_index (unused by VoiceListener)

------------------------------------------------------------
DESIGN GUIDELINES FOR AUDIO MODULE
------------------------------------------------------------

1) Keep streaming APIs async (use AsyncIterable[bytes | str]).
2) Avoid blocking UI threads; use background thread like VoiceListener does.
3) Always normalize transcripts (trim, lowercase if needed).
4) Handle provider failures gracefully and return a clear error.
5) Log the provider used and timing if log_command_debug is enabled.

------------------------------------------------------------
TESTING / MANUAL CHECKS
------------------------------------------------------------

Basic local test:
- Create a VoiceListener with a CommandController.
- Call start() and say a simple command like "open chrome".
- Confirm controller receives transcript and prints command logs.

If using OpenAI Realtime:
- Ensure local Whisper model is available or configured.
- Whisper runs locally via faster-whisper.

------------------------------------------------------------
SUMMARY
------------------------------------------------------------

To integrate STT results into the command system, you must call:
CommandController.handle_event(source="voice", action=transcript)

Everything else (UI controls, API endpoints, provider selection) is optional
and can be layered on top.
